Created: 120819



current:


    - design the multiarmed bandit problem using the RLTrade architecture.
        ... this will help solidify the current framework workflow and responsibility distribution (which you should record)
            ... doing this should help highlight the areas that need refining, and how best to approach it for generalized functionality
            ... these areas include - where to use ABCs, how to handle reporting overall (this should be transfered from environment to experiment class),
                ... and how to evaluate agent data.
                    - maybe a reporting/visualization super class can be used to inherit for specific agents (and other classes if needed)
                        this would enable modular reporting which handles different types of agents, depending on the reporting characteristics of interest.
                - search for ways to clearly define the ultimate functionality that the framework is intended to handle/provide to general RL dev process
                    -a pipline adds useful structures such as reporting, and modular state/action/reward groups, while remaining flexible enough to be applied to most RL problems

    - Agent interface should require self reporting such that it will log it's own predictions when the "act" method is called.
        ... this will help prevent excess slow down by querying the network in a separate location, essentially having two calls per step.

    - Now that we're logging agent data, create the visualization.

    - remember that this is for you and the goal is to create a flexible sandbox environment for developing and testing trading-related agents.
        ... don't be afraid to sacrifice abstration because you might end up using the higher level components as a general purpose RL framework.
        -Make the tools and things that are fun and effective.

    - create a modular state and reward function library
        ... should have an individual function that obtains specific rewards or states
        ... these functions can just be swaped in and out of the enironement's getReward and getState methods.
            ... a predefined format would allow you to define a convention for returning metadata (i.e. a brief description) for each function.
                -e.g. each has "asDescription=False" param, which, when true, returns the functions str descriptor
        - consider creating classes to contain the component functions 
            -the main points of these would be 
                -1. to introduce structure that allows a consistent way to track metadata from different states/actions/reward without requiring the user to
                    ... subclass a new environment and/or constantly update metadata for every change to these groups..
                -2 to allow for systematic interchangability during experiment runtimes and being able to track those changes, and have accurate metadata throughout the experiment
            -consider using "component container" super class?... 
                -this can handle basic methods regarding getting metadata
                -possibly can also allow quick setup/usage of component groups.. then we just reference the component group in the reward/state/action functions
            -this may be more efficient, as you will be able to provide the class with references to the objects they need instead of needing to worry about passing it
                -this could be done by mapping arguments, including object references, to strings in a dict that the class has - and then mapping the params that each function needs....
                    ... at runtime, each function will be called and the arg strings will 
        - Most of this has been implemented - Need to unify everything such that groups of states/rewards/actions can be made and processed by the environment class, utilizing the metaData functionality
            -Two possible approaches (briefly described above)
                -1. Use RLComponent class to package individual functions, adding properties to handle metadata... then, package these RLComponents into ComponentGroup classes for plugging into the environment.
                    -an issue here is that having the function's metadata be handled in the class limits user's ability to create custom functionality without subclassing.. which would be messy to have a bunch of
                        ... components instantiated and then 1 or 2 randoms that need to be subclassed..
                            -this is resolvable by expecting/allowing metadata from the callback function (i.e. using the getMetaData param which is currently used in )
                    -consider having swappable parameter "sets" for RLComponent - and for ComponentGroups, you can enable/disable various components and maybe store/swap presets of enabled groups
                    -consider a way to have access to RL objects (env,experiment,etc) as default in these classes.. then user can reference them in callbacks without worrying about setting them up in the argList
                -2. Only use ComponentGroup class and design it to directly reference callback functions, or overridden functions as a subclass

                -Note - Either of the above might be uneccessary extra work and hinder the flow, while also de creasing experiment speed & increasing memory.
                -To avoid using Component classes, you can just keep the component metadata structure in the state/action/reward methods, and implement functions in the Environment class that access that data, or
                    ... you can simply provide a method where the use can provide all neccessary metadata (i.e. the experiment.metadata dict)
                    - 122819  Am testing the RLComponent approach (1):
                        -Create RLComponent Class
                        -Create RLComponentGroup Class
                        -Create Action/StateFeature/Reward group classes
                        -Edit TradingEnvironment to expect and use component groups
                            -creating ProjectComponents/"group" files
                                -already finished "actionGroups"
                                -trying to figure out if there is a better way to combine reward functions other than creating compound functions `<- Need to return to this`
                                -once finished setting up the groups, continue to setup a test using the componentgroups.
                                    - Found out when setting up first RLComponent Test, that the instantiation is kinda messy
                                        -Not sure what process I was thinking of back when I started the RLComponent approach, but I had the env require the component sets, and made the
                                            ...factory component methods require the env as an arg.. so as of now, there are two obvious methods to setup components -
                                            ...1.) pass unbound methods that reference the RlComponent set factory methods (e.g. stateSets.stateSet1 ) to env, and then env will call the method, passing itself during init
                                            ...2.) remove requirement for componentSet args on env.init and then just set them after instantiating env.
                                            -Going with #2 for now.

                            -currently just working with old TradingEnvironment class (without inheriting RLEnv) to get components working - then you can move to the lines below.
                            `*Paused here*`
                            -create environment super class
                                -move reporting features to Experiment
                                    -skipping this for now .. look into decorators (and property decorator) to see if it might provide a better solution for reporting
                            -recreate Trading Environment sub class
                        -Edit component methods to expect env as first param(instead of trader), and try creating component groups
                        - Alternative approach to RLComponent classes - See if you can use decorators to assign a function as an "action function", "reward function", etc. 
                            -maybe the decorator can then handle the RLComponent group behavior behind the scenes rather than forcing the user to setup individual component classes, and then the group classes.

    
    - need to rename the default test data columns to from o h l c to open high low close.
        -will need to account for these changes in Asset, state features, assetDataLib, etc.
        -this is for compatibility with the talib.abstract functions


    - finish implementing experiment.details 
        ... needs to be added to console log
        ... may need it's own modify method.


    - design framework for agents that choose their own state space
        ... This could start just with a framework for easily applying TA to any asset,
            ... then, create a system that either generates state spaces, or generates specialized agents that come with prepacked "sensors" for interpreting an asset, i.e. a specific set of TA indicators and params
                ...they could then be evaluated independetly, or the optimizing system can utilize a genetic/evolution approach.

    - Look into initializing network weights and optionally  avoiding zero as input 


    `** paused here 022320 - working below on clusters & normalization **`
    - Working on implementing TA-LIB for easy ta data
        -working in talib_scratchpad.py
        - How should ta data be available?
            - On the fly as needed with a single call, only generating the data as its called?
                -can maybe use talib.abstract to 
            - Generate data each bar from a set of indicators "assigned" to the asset? (eg. )
        - consider a general method to normalize data derived from price as to make it agnostic to values in each asset (i.e. training on an indicator of an asset with price ~$10 won't inhibit the model from working on an asset worth $5. e.g. the slope calculation of an sma depends on sma values which depend on price.)

        - need to design way to easily construct statespace features that convey asset behavior over a period
        -You should be able to easily choose which features of an asset you want to include, and how many bars in the past you want these features to span
        -This will allow the agent a recent input over a period to sense change rather than just a snapshot at the present
        
        - There should also be a standard method by which the asset features are converted to relational data
            -e.g. all feature data is filtered through stochastic oscillators

    `** YOU ARE HERE **`

    - working on ensuring the NN has normalized input
        -working in the normalization scratchpad to test techniques that can normalize incoming datapoints
            ... around the same scale as the existing data (and how to handle the first bits of data)
            -Look into minmaxscaling and other scaling/standardizing techniques 
        - Where to implement the normalization
            -I'm going to set it up inside the DQNAgent for now.. although this may not be the best class for it.
                -the agent will normalize the data in the self.remember method as it comes in, and also in the replay method.
                -it will use it's "memory" as reference when scaling new datapoints.
    
    - look into using clustered categories as state inputs
        -run statefeature observations through unsupervised learning (e.g. clustering) model to group similar asset states... the generalization between states should eliminate complexity for the agent.
            -ideas on models that don't require specifying clusters. https://stats.stackexchange.com/questions/241381/clustering-methods-that-do-not-require-pre-specifying-the-number-of-clusters 
                -possible issue -> if the number of state clusters grows over time, the agent's old information may become obsolete (if a states previously clustered as group 0 start shifting to group 1)
                    -solutions
                        -probably could counter this via epsilon control
                        -also can just do some extra wiring to make sure we can track the changes being made to the clustering model, and provide an interface for the agent to adapt to them.
                            ... e.g. after a new cluster is added, some old states shift around, and the agent knows going forward that examples of group 0 previously seen may now be shown as group 1, etc
        - Developing cluster visualizations: @done
            -I think one helpful way of visualizing clusters of state features would be to view the test features on a graph
                ... i.e. regular price graph with all the indicators being used as features, and then color the background for each cluster label
                ... The cluster labels are assigned in order to each bar upon fitting, so you can use the labels when plotting the graph to 
                ... determine the color that the background should be.

        - start tests with different value types (bool, int,float)
            - i've read that Gower's similarity coefficient will help here
            - Also, try bools as ints and just bools

        
        - After finding a suitable cluster method (one that can adapt and grow in a live env), create a stateSet using the model
            - The cluster model main need to be pretrained
                -maybe we could start it live, and adjust the weight of that input down until the clusters stabilize.
                    -However, is there really a good reason not to just pretrain it before using it?
        - When testing, the agent should have one or more clusterers each feeding a single label data to a single input
            -This will allow info to be more easily digested by the network since the complex asset feature analysis is
                ... being performed by a separate component.
            - Use multiple clusters models each responsible for monitoring, and categorizing specific datastreams/features
                -e.g. one clustermodel takes multiple SMA data in and outputs a class label which is a single input for the NN
                    ... while another is doing the same, but for different indicators.
                    
                    -This approach will increase reliability, convergance, and performance of the cluster models rather than
                        ... using a single model to handle all the asset features.

    - look into metric learning


    - Sudden issue with stateSize? (Fixed - But not resolved.)
        -all of a sudden, i'm getting errors when trying to use state sets that have features with nan values.
            ... the error pops up when the nextState is being reshaped for input into the NN.
            ... I'm not sure what's causing this - I'm pretty sure this is related to nan values bc the error does not arise
            ... when non nan-valued state features are used, but I've used these state sets previously without issue.. so something else has to be happening.
            ... Also supporting the idea that the nan values are currently triggering the issue is the fact that I avoid the problem by using the marketStartBarNum 
            ... to start the sim after the nan values, and the error does not occur.
            ...     This is not a fix, however, bc I have notes to leave the marketStartBarNum at 0 bc of unresolved issues.
            ... I've used these 
        -Fix: I fixed this by using the marketStartbarNum to avoid MA data (and resolved the isse with that in the process)
            ... However, still not sure what actually caused this to occur in the first place or what changed that stopped it from working as it did previously

later:

    - MarketStartBar != 0 feature causes issues @done
        -using marketStartBar should allow for a "warmup period"
        -However, the offset is causing some issues which are visible when using the report visualizer.. all the trades are offset from the actual price.
            -Find out if this is strictly a vizualization problem, or something more.
        -Solved: This looks like it was mainly an issue with just the way the report was being handled.
        

    - Verify stability of statefeature component functions which return iterables.
        -I just modified StateFeatureSet class to all it's component functions to return a list of values instead of a single return value per function
        ... Everything appears to work still, but I am not certain that this change wont cause issues somewhere since I don't think I had this feature in mind when developing the RLcomponent structure.
    

    - Better epsilon decay algorithm *important*
        -Right now, epsilon decay is done arbitrarily during each replay call (once per call, not per replay loop in each call)
        ... there is for sure a better approach to this. Keep in mind epsilon is how much the model believes in it's current policy.
            -Increase/reduce epsilon as a function of recent performance consistency given certain states.
            -Multiple epsilon values?
                -one for each action?
                -one for each action/state pair?
                    -How
                        -use unsupervised learning like KNN to group states into categories?
                    -Why
                        -this would allow for smarter choices.. and better adaptive behavior.
                        ... e.g. if it knows a given action in a given state always is good, it's decision to take that action should not be affected
                        ... by the fact that it's not as sure about other states, which is exactly what happens when all actions are governed by a single epsilon value 

    - need to test Asset class' getDataFunction for getting all data (should be allowed when barNum=None)
        -seems like I am getting a return of None when I should be getting all bar data.. need to verify/test..super tired.

    - look into Google's Dopamine, bc somehow I haven't seen this and it looks like they're going for a similar set of features with that framework

    - DQNAgent -> test if replay_v2 is better performing than orig
        I feel like reward * gamma * maxQ makes alot more sense as a target calculation when reward is arbitrary
        -Update: Actually, i looked up the algorithm and I guess the way I had it before is correct.

    - DQNAgent should be able to log when a random action is taken in .act(), as opposed to using the model

    - RLTradeReportPlotter.plotActions should use action names as y axis, or include legend

    - allow experiments to reset and run multiple times, using the same agent
        -consider allowing the option of using the same agent or not
        -This funcitonality exists in current main.py

    - consider using abstract base classes (abc module) to create interfaces for the various main classes 

    - May want to move 'positionSizeIncrement' attribute from TradingEnv to TradingAccount

    - TradingAccount.enterPosition() - Reversal feature
        -was about to put this in place, but realized that would log two position entries on a single bar, which won't be compatible with some things
            -the only thing I can think of now is the "getLastClosedTrade" features - this will need to be updated to search for the most recent closed trade rather than
                ... just using the most recent index.
            -Commented out the enterPosition() method with reversal functionality until this is taken care of.

    - Add structured meta data as a feature on the Experiment class
        -For example, require that the user provides specific information about state features and rewards used so that this information is retained and can be observed/compared
            ... when viewing a specific experiement's results.
            -This could be implemented by providing specific callbacks or overriding methods dedicated to getting the description in a specific format.
            -or, maybe, the relevant files (experiment and env subclasses,.. maybe somehow the separate reward libraries,etc ) are saves and copied into the experiment's dir.
                ... then, those files are used when loading an experiment.
                ... Similarly, all agent models run during under those specific experiment params will be saved in the experiment dir.
                    ... This might have to be reconsidered (or just overridden in subclass) if implementing dynamic experiment params, e.g. in the case of evolving state spaces or multi agent experiments.

    - consider how to refactor reporting (and maybe visualization)

    - setup parallel processing using multiprocessing module;
        - also, TFF has distributed training which can take advantage of multiple GPU

    - Tests show that the time to complete each episode generally increases.. need to find out why and determine severity.
        ... complexity issue?
        ... try tests while monitoring cpu temp. 

    - work through flow to determine when accountLog is updated in TradingAccount.. 
        ... trying to determine at what points it is safe to reference the account log for most current info.. e.g. if I want to see what my account value was at the previous bar, i want to make sure I'm
        ... actually accessing the correct index in the accountLog.

    - write out quick use case/mini docs on how to go about designing and setting up an experiment
        ...this should help highlight any flow issues and refresh your high level perspective

    - refactor DQNAgent

    - consider an experimentreporting class, or just a single dataframe which is passed to the various classes who each can handle their own reporting at a given bar?.. 

    - batch size should be contained in the DQN agent class

    - should probably extract the experiment-agnostic features from TradeEnv, and create a custom Environment super class - DeepwellEnvironemnt?

    - change "episode" naming convention to epoch
        -maybe? i think actually the episode convention may be correct.

    - rename TradePosition.getPositionValue() to just "getValue()"

    - rethinking "custom user methods" dict approach in RLExperiment ... probably better to just subclass and add features via overriding... but test these anyway.. might be worth keeping the functionality
    ... But if you keep it, you may need to add similar functionality to Environment for consistent behavior

    - set RLTrade package up as a proper module so we can import from a parent dir
        ... need to resolve "attempted relative import with no known parent package" error

    - Create an extension for Market class (or asset), which provides access to a technical analysis library, allowing user to get or generate TA data inherently, without needing to provide it as a feature.
        - may be able to include a list of desired TA features in a list as callbacks when initiailizing Market, and those TA data will be included in the dataframes of all assets
            - You'd have to handle cleaning (removing NAN from begininng, or just handling NAN for indicators which generate sporadic NANs through the data)

Performance(speed) improvements:
    - try using tf.data.dataset API to increase training speed.
        -would need to replace the "memory" datastructure (currently deque) in DQNAgent class
    - add other gpu and implement tf's distributed training


later non-mvp:
    General:
        - Utilize tradingview library for visualization
        - integrate into quantconnect
        - design built in support for dynamic action spaces. 
        ... e.g. based on a flag or callback condition, a different set of actions can be available to the agent.
        ... This wouldn't work with the existing DQNAgent since the networks size is dependent on the action space.. but you could maybe set the size to that of the largest action set, and have a unique acionId 
        ... for each action across all sets..(actually, you could just use a single large action set) then just limit the agent to the desired actions as needed.. e.g. allPossibleActionsSet = [0,1,2,3,4]; but in
        ... a given state, agent might only be able to choose from [2,3,4] or [1,4], etc.

        - probably should build an Agent interface.. maybe geared toward DQN, so that modifying the model can be done without subclassing... or just has the basic features, and you provide it the model.
        ... an agent "constructor" would be cool... you can construct an agent class, or subclass, (or instance!) from modular parts with the features you want.
        ... e.g. does this agent use replay? If so, replay and remember functions are added....   Being able to define General architecture would be super cool.
        ... we definitely could create functions that spit out a desired NN architecture.. but 

        - need to design testing process flow; ability to easily test a given agent on unseen data.

    TradingAccount:
        - redesign margin checks

    


non-mvp feature brainstorm:

    - might want to consider redesigning as lightweight as possible - You're currently trying to making things as robust and general as possible (as usual), but that is adding overhead.
        ... e.g., extra loops and stuff where there doesn't need to be... there is a trade off between user friendliness and efficiency; for example, in the Market class; everytime you call getAssetDataFeatures(),
        ... it will validate the assetIds, whether they're names, id's or a combination. This is cool and nice.. but unneccesary.. especially during development. It's better to understand and comply to a specific
        ... expected parameter and streamline performance.

    RLExperiment:
        - consider changing "done" and "onDone" to something like "onEpisodeFinish"

    TradingAccount:
        - allow getRealizedProfit to have a custom window - I was doing this now, but am skipping to not get bogged down.. also don't have a use case in mind where this is needed during mvp
            ... this should use the built in account log
            `def getRealizedPL(self, startBarNum=0, endBarNum=None)
            """ return difference in current accountValue(excluding unrealizedPL) from startBarNum to endBarNum; if endBarNum==None, current bar is used """
                if endBarNum==None: endBarNum = self.market.currentBarNum
                return ...
    
    testDataGen:
        - eventually, we should be experimenting with using GANs to increase training data volume... this could be super useful if training with images, too.
        
        - 


Things User should know:
    RLExperiment:
        - whenever self.done == True, the currentEpisode will end.
        - if the rewardDict does not contain a reward for the current eventFlag, a reward of 0 will be returned;
        - in most cases, event flags should be used to indicate meta-events, or episode-halting events which might need to be accounted for in your reward, even though they may not be contained in the state observation.
            ... you mostly don't want to set them strictly to determine reward provided from the reward dict (although, you could). A better approach, would be to call a reward function from the getRewardDict() method
            ... and do all neccesary state/condition calculations in that reward func.
        - you probably want to override setStopFlag()



done: 

    
    ✔ Important -> Need to review code for this:: checked files for issues... @done(20-02-02 16:15)
    -Turns out that the odd shared variable behavior was a result of python using the same memory for a list being used as a default param in the method's signature
        ... i.e. if you use a mutable type as a default argval, python will evaluate/create that default arg once when it is instantiated, NOT each time it's called.
        ... I had no idea about this and probably used similar structure elsewhere..
        -replace all mutable default argvals with None, then set as needed inside the function


    ✔ The TradeEnv class seems about ready to be subclassed. @done(19-12-12 21:23)
    ✔ You need to determine any tweaks that need to be made to TradingAccount and Market. @done(19-12-14 22:34)
        TradingAccount seems like it already has everything needed to be passed to TradeEnv as it's "trader".

    ✔ update for below: Experiment class started - you should make the super class general, and then subclass for a TradingExperiment,, which expects and handles a by-bar training loop. @done(19-12-12 19:44)
        -You may still want to create an Experiment class which will contain the current main.py loop in a "run()" function, or something.
        -takes an agent and the env, and anything else required (e.g. nEpisodes)
        and handles the actual training process.
            -this class would provide the structure for training, and 
            handle all the extra stuff like terminal updates, timing, model saving, etc.
            -This could also be the uppermost class to handle reporting, taking it
            away from the environment responsibilities. 
                -maybe env should still keep a relevant log, but experiment handles exporting and naming, etc.

            -should have either an overridable method or user can provide a callable method
            ... which is called at various parts of training to add custom functionality.
            ... e.g. on the "if done" part, or after each epoch.. that would allow the generic experiment class to still
            ... allow for things like calling replay on agents that need it, etc.

    ==
    sidenote
    12/8/19 9:55PM
    The deepwell framework is concieved.
    ==

    ✔ finish RLExperiment abstract class .. should be compatible with @done(19-12-12 19:46)
        ...any generic RL training task (or most, don't get stuck)
        -(done)add user callback functions

    ✔ then, finish RLTradingExperiment class.. should inherit RLExperiment @done(19-12-12 19:47)
        ...and the run function should accomplish everything from v2's main.py loop.



    ✔ continue reconstructing main.py in new v3 architecture and test until it works @done(19-12-14 21:29)
        ✔ test the userMethodDict approach and the subclass approach @done(19-12-14 22:35)
            - for the subclass approach, you can create a reusable subclass for use with the DQNAgent.