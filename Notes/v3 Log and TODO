Created: 120819



current:

   


    - design the multiarmed bandit problem using the RLTrade architecture.
        ... this will help solidify the current framework workflow and responsibility distribution (which you should record)
            ... doing this should help highlight the areas that need refining, and how best to approach it for generalized functionality
            ... these areas include - where to use ABCs, how to handle reporting overall (this should be transfered from environment to experiment class),
                ... and how to evaluate agent data.
                    - maybe a reporting/visualization super class can be used to inherit for specific agents (and other classes if needed)
                        this would enable modular reporting which handles different types of agents, depending on the reporting characteristics of interest.
                - search for ways to clearly define the ultimate functionality that the framework is intended to handle/provide to general RL dev process
                    -a pipline adds useful structures such as reporting, and modular state/action/reward groups, while remaining flexible enough to be applied to most RL problems

    - Agent interface should require self reporting such that it will log it's own predictions when the "act" method is called.
        ... this will help prevent excess slow down by querying the network in a separate location, essentially having two calls per step.

    - Now that we're logging agent data, create the visualization.

    - remember that this is for you and the goal is to create a flexible sandbox environment for developing and testing trading-related agents.
        ... don't be afraid to sacrifice abstration because you might end up using the higher level components as a general purpose RL framework.
        -Make the tools and things that are fun and effective.

    - create a modular state and reward function library
        ... should have an individual function that obtains specific rewards or states
        ... these functions can just be swaped in and out of the enironement's getReward and getState methods.
            ... a predefined format would allow you to define a convention for returning metadata (i.e. a brief description) for each function.
                -e.g. each has "asDescription=False" param, which, when true, returns the functions str descriptor
        - consider creating classes to contain the component functions 
            -the main points of these would be 
                -1. to introduce structure that allows a consistent way to track metadata from different states/actions/reward without requiring the user to
                    ... subclass a new environment and/or constantly update metadata for every change to these groups..
                -2 to allow for systematic interchangability during experiment runtimes and being able to track those changes, and have accurate metadata throughout the experiment
            -consider using "component container" super class?... 
                -this can handle basic methods regarding getting metadata
                -possibly can also allow quick setup/usage of component groups.. then we just reference the component group in the reward/state/action functions
            -this may be more efficient, as you will be able to provide the class with references to the objects they need instead of needing to worry about passing it
                -this could be done by mapping arguments, including object references, to strings in a dict that the class has - and then mapping the params that each function needs....
                    ... at runtime, each function will be called and the arg strings will 
        - Most of this has been implemented - Need to unify everything such that groups of states/rewards/actions can be made and processed by the environment class, utilizing the metaData functionality
            -Two possible approaches (briefly described above)
                -1. Use RLComponent class to package individual functions, adding properties to handle metadata... then, package these RLComponents into ComponentGroup classes for plugging into the environment.
                    -an issue here is that having the function's metadata be handled in the class limits user's ability to create custom functionality without subclassing.. which would be messy to have a bunch of
                        ... components instantiated and then 1 or 2 randoms that need to be subclassed..
                            -this is resolvable by expecting/allowing metadata from the callback function (i.e. using the getMetaData param which is currently used in )
                    -consider having swappable parameter "sets" for RLComponent - and for ComponentGroups, you can enable/disable various components and maybe store/swap presets of enabled groups
                    -consider a way to have access to RL objects (env,experiment,etc) as default in these classes.. then user can reference them in callbacks without worrying about setting them up in the argList
                -2. Only use ComponentGroup class and design it to directly reference callback functions, or overridden functions as a subclass

                -Note - Either of the above might be uneccessary extra work and hinder the flow, while also decreasing experiment speed & increasing memory.
                -To avoid using Component classes, you can just keep the component metadata structure in the state/action/reward methods, and implement functions in the Environment class that access that data, or
                    ... you can simply provide a method where the use can provide all neccessary metadata (i.e. the experiment.metadata dict)
                    - 122819  Am testing the RLComponent approach (1):
                        -Create RLComponent Class
                        -Create RLComponentGroup Class
                        -Create Action/StateFeature/Reward group classes
                        -Edit TradingEnvironment to expect and use component groups
                        `** YOU ARE HERE **`
                            -creating ProjectComponents/"group" files
                                -already finished "actionGroups"
                                -trying to figure out if there is a better way to combine reward functions other than creating compound functions
                                -once finished setting up the groups, continue to setup a test using the componentgroups.
                            -currently just working with old TradingEnvironment class (without inheriting RLEnv) to get components working - then you can move to the lines below.
                            -create environment super class
                                -move reporting features to Experiment
                                    -skipping this for now .. look into decorators (and property decorator) to see if it might provide a better solution for reporting
                            -recreate Trading Environment sub class
                        -Edit component methods to expect env as first param(instead of trader), and try creating component groups
                        - Alternative approach to RLComponent classes - See if you can use decorators to assign a function as an "action function", "reward function", etc. 
                            -maybe the decorator can then handle the RLComponent group behavior behind the scenes rather than forcing the user to setup individual component classes, and then the group classes.


    - finish implementing experiment.details 
        ... needs to be added to console log
        ... may need it's own modify method.


    - design framework for agents that choose their own state space
        ... This could start just with a framework for easily applying TA to any asset,
            ... then, create a system that either generates state spaces, or generates specialized agents that come with prepacked "sensors" for interpreting an asset, i.e. a specific set of TA indicators and params
                ...they could then be evaluated independetly, or the optimizing system can utilize a genetic/evolution approach.


later:

    - RLTradeReportPlotter.plotActions should use action names as y axis, or include legend

    - allow experiments to reset and run multiple times, using the same agent
        -consider allowing the option of using the same agent or not
        -This funcitonality exists in current main.py

    - consider using abstract base classes (abc module) to create interfaces for the various main classes 

    - May want to move 'positionSizeIncrement' attribute from TradingEnv to TradingAccount

    - TradingAccount.enterPosition() - Reversal feature
        -was about to put this in place, but realized that would log two position entries on a single bar, which won't be compatible with some things
            -the only thing I can think of now is the "getLastClosedTrade" features - this will need to be updated to search for the most recent closed trade rather than
                ... just using the most recent index.
            -Commented out the enterPosition() method with reversal functionality until this is taken care of.

    - Add structured meta data as a feature on the Experiment class
        -For example, require that the user provides specific information about state features and rewards used so that this information is retained and can be observed/compared
            ... when viewing a specific experiement's results.
            -This could be implemented by providing specific callbacks or overriding methods dedicated to getting the description in a specific format.
            -or, maybe, the relevant files (experiment and env subclasses,.. maybe somehow the separate reward libraries,etc ) are saves and copied into the experiment's dir.
                ... then, those files are used when loading an experiment.
                ... Similarly, all agent models run during under those specific experiment params will be saved in the experiment dir.
                    ... This might have to be reconsidered (or just overridden in subclass) if implementing dynamic experiment params, e.g. in the case of evolving state spaces or multi agent experiments.

    - consider how to refactor reporting (and maybe visualization)

    - setup parallel processing using multiprocessing module;
        - also, TFF has distributed training which can take advantage of multiple GPU

    - Tests show that the time to complete each episode generally increases.. need to find out why and determine severity.
        ... complexity issue?
        ... try tests while monitoring cpu temp. 

    - work through flow to determine when accountLog is updated in TradingAccount.. 
        ... trying to determine at what points it is safe to reference the account log for most current info.. e.g. if I want to see what my account value was at the previous bar, i want to make sure I'm
        ... actually accessing the correct index in the accountLog.

    - write out quick use case/mini docs on how to go about designing and setting up an experiment
        ...this should help highlight any flow issues and refresh your high level perspective

    - refactor DQNAgent

    - consider an experimentreporting class, or just a single dataframe which is passed to the various classes who each can handle their own reporting at a given bar?.. 

    - batch size should be contained in the DQN agent class

    - should probably extract the experiment-agnostic features from TradeEnv, and create a custom Environment super class - DeepwellEnvironemnt?

    - change "episode" naming convention to epoch

    - rename TradePosition.getPositionValue() to just "getValue()"

    - rethinking "custom user methods" dict approach in RLExperiment ... probably better to just subclass and add features via overriding... but test these anyway.. might be worth keeping the functionality
    ... But if you keep it, you may need to add similar functionality to Environment for consistent behavior

    - set RLTrade package up as a proper module so we can import from a parent dir
        ... need to resolve "attempted relative import with no known parent package" error

    - Create an extension for Market class (or asset), which provides access to a technical analysis library, allowing user to get or generate TA data inherently, without needing to provide it as a feature.
        - may be able to include a list of desired TA features in a list as callbacks when initiailizing Market, and those TA data will be included in the dataframes of all assets
            - You'd have to handle cleaning (removing NAN from begininng, or just handling NAN for indicators which generate sporadic NANs through the data)
    


later non-mvp:
    General:
        - Utilize tradingview library for visualization
        - integrate into quantconnect
        - design built in support for dynamic action spaces. 
        ... e.g. based on a flag or callback condition, a different set of actions can be available to the agent.
        ... This wouldn't work with the existing DQNAgent since the networks size is dependent on the action space.. but you could maybe set the size to that of the largest action set, and have a unique acionId 
        ... for each action across all sets..(actually, you could just use a single large action set) then just limit the agent to the desired actions as needed.. e.g. allPossibleActionsSet = [0,1,2,3,4]; but in
        ... a given state, agent might only be able to choose from [2,3,4] or [1,4], etc.

        - probably should build an Agent interface.. maybe geared toward DQN, so that modifying the model can be done without subclassing... or just has the basic features, and you provide it the model.
        ... an agent "constructor" would be cool... you can construct an agent class, or subclass, (or instance!) from modular parts with the features you want.
        ... e.g. does this agent use replay? If so, replay and remember functions are added....   Being able to define General architecture would be super cool.
        ... we definitely could create functions that spit out a desired NN architecture.. but 

        - need to design testing process flow; ability to easily test a given agent on unseen data.

    TradingAccount:
        - redesign margin checks

    


non-mvp feature brainstorm:

    - might want to consider redesigning as lightweight as possible - You're currently trying to making things as robust and general as possible (as usual), but that is adding overhead.
        ... e.g., extra loops and stuff where there doesn't need to be... there is a trade off between user friendliness and efficiency; for example, in the Market class; everytime you call getAssetDataFeatures(),
        ... it will validate the assetIds, whether they're names, id's or a combination. This is cool and nice.. but unneccesary.. especially during development. It's better to understand and comply to a specific
        ... expected parameter and streamline performance.

    RLExperiment:
        - consider changing "done" and "onDone" to something like "onEpisodeFinish"

    TradingAccount:
        - allow getRealizedProfit to have a custom window - I was doing this now, but am skipping to not get bogged down.. also don't have a use case in mind where this is needed during mvp
            ... this should use the built in account log
            `def getRealizedPL(self, startBarNum=0, endBarNum=None)
            """ return difference in current accountValue(excluding unrealizedPL) from startBarNum to endBarNum; if endBarNum==None, current bar is used """
                if endBarNum==None: endBarNum = self.market.currentBarNum
                return ...
    
    testDataGen:
        - eventually, we should be experimenting with using GANs to increase training data volume... this could be super useful if training with images, too.
        
        - 


Things User should know:
    RLExperiment:
        - whenever self.done == True, the currentEpisode will end.
        - if the rewardDict does not contain a reward for the current eventFlag, a reward of 0 will be returned;
        - in most cases, event flags should be used to indicate meta-events, or episode-halting events which might need to be accounted for in your reward, even though they may not be contained in the state observation.
            ... you mostly don't want to set them strictly to determine reward provided from the reward dict (although, you could). A better approach, would be to call a reward function from the getRewardDict() method
            ... and do all neccesary state/condition calculations in that reward func.
        - you probably want to override setStopFlag()



done: 

    ✔ The TradeEnv class seems about ready to be subclassed. @done(19-12-12 21:23)
    ✔ You need to determine any tweaks that need to be made to TradingAccount and Market. @done(19-12-14 22:34)
        TradingAccount seems like it already has everything needed to be passed to TradeEnv as it's "trader".

    ✔ update for below: Experiment class started - you should make the super class general, and then subclass for a TradingExperiment,, which expects and handles a by-bar training loop. @done(19-12-12 19:44)
        -You may still want to create an Experiment class which will contain the current main.py loop in a "run()" function, or something.
        -takes an agent and the env, and anything else required (e.g. nEpisodes)
        and handles the actual training process.
            -this class would provide the structure for training, and 
            handle all the extra stuff like terminal updates, timing, model saving, etc.
            -This could also be the uppermost class to handle reporting, taking it
            away from the environment responsibilities. 
                -maybe env should still keep a relevant log, but experiment handles exporting and naming, etc.

            -should have either an overridable method or user can provide a callable method
            ... which is called at various parts of training to add custom functionality.
            ... e.g. on the "if done" part, or after each epoch.. that would allow the generic experiment class to still
            ... allow for things like calling replay on agents that need it, etc.

    ==
    sidenote
    12/8/19 9:55PM
    The deepwell framework is concieved.
    ==

    ✔ finish RLExperiment abstract class .. should be compatible with @done(19-12-12 19:46)
        ...any generic RL training task (or most, don't get stuck)
        -(done)add user callback functions

    ✔ then, finish RLTradingExperiment class.. should inherit RLExperiment @done(19-12-12 19:47)
        ...and the run function should accomplish everything from v2's main.py loop.



    ✔ continue reconstructing main.py in new v3 architecture and test until it works @done(19-12-14 21:29)
        ✔ test the userMethodDict approach and the subclass approach @done(19-12-14 22:35)
            - for the subclass approach, you can create a reusable subclass for use with the DQNAgent.